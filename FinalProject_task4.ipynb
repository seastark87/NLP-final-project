{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: 2021년 국립국어원 인공지능 언어능력 평가\n",
    "\n",
    "- [2021년 국립국어원 인공지능 언어능력 평가](https://corpus.korean.go.kr/task/taskList.do?taskId=1&clCd=END_TASK&subMenuId=sub01) 는 9월 1일부터 시작하여 11월 1일까지 마감된 [네 가지 과제에](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02) 대한 언어능력 평가 대회\n",
    "- 여기서 제시된 과제를 그대로 수행하여 그 결과를 [최종 선정된 결과들](https://corpus.korean.go.kr/task/taskLeaderBoard.do?taskId=4&clCd=END_TASK&subMenuId=sub04)과 비교할 수 있도록 수행\n",
    "- 아직 테스트 셋의 정답이 공식적으로 공개되고 있지 않아, 네 가지 과제의 자료에서 evaluation dataset으로 가지고 성능을 비교할 계획\n",
    "- 기말 발표전까지 정답셋이 공개될 경우 이 정답셋을 가지고 성능 검증\n",
    "- Transformers 기반 방법론, 신경망 등 각자 생각한 방법대로 구현 가능\n",
    "- 현재 대회기간이 종료되어 자료가 다운로드 가능하지 않으니 첨부된 자료 참조\n",
    "- 개인적으로 하거나 최대 두명까지 그룹 허용. \n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 FinalProject_[DS또는 CL]_학과_이름.ipynb\n",
    "- 마감 12월 6일(월) 23:59분까지.\n",
    "- 12월 7일, 9일 기말 발표 presentation 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리더보드\n",
    "\n",
    "- 최종발표전까지 각조는 각 태스크별 실행성능을 **시도된 여러 방법의 결과들을 지속적으로**  [리더보드](https://docs.google.com/spreadsheets/d/1-uenfp5GolpY2Gf0TsFbODvj585IIiFKp9fvYxcfgkY/edit#gid=0)에 해당 팀명(구성원 이름 포함)을 입력하여 공개하여야 함. \n",
    "- 최종 마감일에 이 순위와 실제 제출한 프로그램의 수행 결과를 비교하여 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. 판정 의문문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-2nyvz4dz\n",
      "Requirement already satisfied: sentencepiece>=0.1.6 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from kobert==0.1.2) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from kobert==0.1.2) (1.9.0)\n",
      "Requirement already satisfied: transformers>=4.8.1 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from kobert==0.1.2) (4.12.5)\n",
      "Collecting gluonnlp>=0.6.0\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "\u001b[K     |████████████████████████████████| 344 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (1.19.5)\n",
      "Requirement already satisfied: cython in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (0.29.17)\n",
      "Requirement already satisfied: packaging in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from gluonnlp>=0.6.0->kobert==0.1.2) (21.3)\n",
      "Collecting mxnet>=1.4.0\n",
      "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 46.9 MB 140 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from mxnet>=1.4.0->kobert==0.1.2) (2.25.1)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting onnxruntime>=0.3.0\n",
      "  Downloading onnxruntime-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (3.14.0)\n",
      "Requirement already satisfied: flatbuffers in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from onnxruntime>=0.3.0->kobert==0.1.2) (1.12)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.1.2) (2020.12.5)\n",
      "Requirement already satisfied: typing_extensions in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from torch>=1.7.0->kobert==0.1.2) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (2.0.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (0.10.3)\n",
      "Requirement already satisfied: filelock in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (4.46.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (0.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from transformers>=4.8.1->kobert==0.1.2) (2021.11.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from packaging->gluonnlp>=0.6.0->kobert==0.1.2) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.1.2) (3.4.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from protobuf->onnxruntime>=0.3.0->kobert==0.1.2) (1.15.0)\n",
      "Requirement already satisfied: click in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.1.2) (1.0.1)\n",
      "Building wheels for collected packages: kobert, gluonnlp\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=13124 sha256=d9bd2d9138f3720b762bb504e2a2fe7712b2cdfb1b9cb7b03d32c96d32225619\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-x6fb2ruw/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595891 sha256=52357a0748b6a05b083effe524b258662dfe83b3c3174fca2611c172a2743b19\n",
      "  Stored in directory: /home/seokilee/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
      "Successfully built kobert gluonnlp\n",
      "Installing collected packages: graphviz, onnxruntime, mxnet, gluonnlp, kobert\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.16\n",
      "    Uninstalling graphviz-0.16:\n",
      "      Successfully uninstalled graphviz-0.16\n",
      "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 kobert-0.1.2 mxnet-1.8.0.post0 onnxruntime-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/seokilee/anaconda3/envs/finance/lib/python3.7/site-packages (0.1.96)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from numpy.core.numeric import Infinity\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import PreTrainedModel,PretrainedConfig\n",
    "from transformers import BertPreTrainedModel,BertModel, AdamW\n",
    "from transformers import ElectraModel,ElectraForSequenceClassification, ElectraTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# koelec\n",
    "class QA_Model(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.max_length = config.max_length\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.bert = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.dense = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.activation = nn.Tanh() \n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) # > 배치, 라벨 수(true or false : 2)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        bert_outputs = self.bert(input_ids,\n",
    "                                 attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        return bert_outputs[0]\n",
    "\n",
    "# roberta\n",
    "class QA_Model1(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.max_length = config.max_length\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-large\")\n",
    "        self.dense = nn.Linear(config.hidden_size,config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.activation = nn.Tanh() \n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) # > 배치, 라벨 수(true or false : 2)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        bert_outputs = self.bert(input_ids,\n",
    "                                 attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        return bert_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# koelec\n",
    "class QA_trainer():\n",
    "    def __init__(self,num_labels,hidden_size,max_length,hidden_dropout_prob,batch_size,epoch,model_ver,learning_rate,weight_decay):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.model_ver = model_ver\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = QA_Model.from_pretrained(self.model_ver,num_labels=self.num_labels,hidden_size=self.hidden_size,hidden_dropout_prob=self.hidden_dropout_prob,max_length=self.max_length)\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.sep_token = self.tokenizer.sep_token\n",
    "    def make_dataset(self):\n",
    "        '''\n",
    "            data =\n",
    "            [\n",
    "            sentence_id : str\n",
    "            Context : str\n",
    "            Question : str\n",
    "            ( True : 1 or False : 0 ) : int \n",
    "            ]\n",
    "        '''\n",
    "        train_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Train.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                last_data = int(re.sub('\\n','',info.split('\\t')[-1]))\n",
    "                pre_data.append(last_data)\n",
    "                train_data.append(pre_data)\n",
    "        val_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Dev.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                last_data = int(re.sub('\\n','',info.split('\\t')[-1]))\n",
    "                pre_data.append(last_data)\n",
    "                val_data.append(pre_data)\n",
    "        test_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Test.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                test_data.append(pre_data)\n",
    "        #train_data\n",
    "        train_data_context = [data[1] for data in train_data]\n",
    "        train_data_question = [data[2] for data in train_data]\n",
    "        train_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([]),'answer' : torch.tensor([])}\n",
    "        for data in range(len(train_data_context)):\n",
    "            train_data_tokenized = self.tokenizer.encode_plus(train_data_context[data],train_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = train_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = train_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = train_data_tokenized['token_type_ids']\n",
    "            train_result['input_ids'] = torch.cat([train_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            train_result['attention_mask'] = torch.cat([train_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            train_result['token_type_ids'] = torch.cat([train_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        train_result['input_ids'] = train_result['input_ids'].long()\n",
    "        train_result['attention_mask'] = train_result['attention_mask'].long()\n",
    "        train_result['token_type_ids'] = train_result['token_type_ids'].long()\n",
    "        train_data_answer = torch.tensor([data[3] for data in train_data])\n",
    "        train_result['answer'] = train_data_answer\n",
    "\n",
    "        #val_data\n",
    "        val_data_context = [data[1] for data in val_data]\n",
    "        val_data_question = [data[2] for data in val_data]\n",
    "        val_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([]),'answer' : torch.tensor([])}\n",
    "        for data in range(len(val_data_context)):\n",
    "            val_data_tokenized = self.tokenizer.encode_plus(val_data_context[data],val_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = val_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = val_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = val_data_tokenized['token_type_ids']\n",
    "            val_result['input_ids'] = torch.cat([val_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            val_result['attention_mask'] = torch.cat([val_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            val_result['token_type_ids'] = torch.cat([val_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        val_result['input_ids'] = val_result['input_ids'].long()\n",
    "        val_result['attention_mask'] = val_result['attention_mask'].long()\n",
    "        val_result['token_type_ids'] = val_result['token_type_ids'].long()\n",
    "        val_data_answer = torch.tensor([data[3] for data in val_data])\n",
    "        val_result['answer'] = val_data_answer\n",
    "\n",
    "        #test_data\n",
    "        test_data_context = [data[1] for data in test_data]\n",
    "        test_data_question = [data[2] for data in test_data]\n",
    "        test_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([])}\n",
    "        for data in range(len(test_data_context)):\n",
    "            test_data_tokenized = self.tokenizer.encode_plus(test_data_context[data],test_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = test_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = test_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = test_data_tokenized['token_type_ids']\n",
    "            test_result['input_ids'] = torch.cat([test_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            test_result['attention_mask'] = torch.cat([test_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            test_result['token_type_ids'] = torch.cat([test_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        test_result['input_ids'] = test_result['input_ids'].long()\n",
    "        test_result['attention_mask'] = test_result['attention_mask'].long()\n",
    "        test_result['token_type_ids'] = test_result['token_type_ids'].long()\n",
    "\n",
    "        train_dataset = TensorDataset(train_result[\"input_ids\"], train_result[\"attention_mask\"],train_result['token_type_ids'],train_result['answer'])\n",
    "        val_dataset = TensorDataset(val_result[\"input_ids\"], val_result[\"attention_mask\"],val_result['token_type_ids'],val_result['answer'])\n",
    "        test_dataset = TensorDataset(test_result[\"input_ids\"], test_result[\"attention_mask\"],test_result['token_type_ids'])\n",
    "\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "    def accuracy(self,predict,label):\n",
    "        predict_answer = predict.argmax(dim=-1)\n",
    "        correct = predict_answer.eq(label.view_as(predict_answer)).sum()\n",
    "        return correct.float() / label.shape[0]\n",
    "\n",
    "    def QA_evaluate(self,model,device,loss_func,data):\n",
    "        model.eval()\n",
    "        epoch_losses = []\n",
    "        epoch_accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in data:\n",
    "            # for batch in tqdm(data,desc='dev_batch'):\n",
    "                input_id = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                token_type_ids = batch[2].to(device)\n",
    "                answer = batch[3].to(device)\n",
    "                predictions = model(input_ids=input_id, attention_mask = attention_mask,token_type_ids=token_type_ids)\n",
    "                loss = loss_func(predictions,answer)\n",
    "                acc = self.accuracy(predictions, answer)\n",
    "                epoch_losses.append(loss.item())\n",
    "                epoch_accs.append(acc.item())\n",
    "        return epoch_losses, epoch_accs\n",
    "\n",
    "    def QA_model_train(self,model,device,optimizer,loss_func,train,print_epoch): #validation 추가\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        for batch in train:\n",
    "        # for batch in tqdm(train,desc='train_batch'):\n",
    "            optimizer.zero_grad()\n",
    "            input_id = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            answer = batch[3].to(device)\n",
    "            predictions = model(input_ids=input_id, attention_mask = attention_mask,token_type_ids=token_type_ids)\n",
    "            loss = loss_func(predictions,answer)\n",
    "            acc = self.accuracy(predictions, answer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        train_loss = epoch_loss / len(train)\n",
    "        train_acc = epoch_acc / len(train)\n",
    "        # print(f'Epoch: {print_epoch+1:02}')\n",
    "        # print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        return train_loss,train_acc\n",
    "\n",
    "    def QA_Train(self,train,val):\n",
    "        epochs = self.epoch\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        valid_losses = []\n",
    "        valid_accs = []\n",
    "        best_valid_acc = float('-inf')\n",
    "        model = self.model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = DataParallel(model)\n",
    "        model = model.to(device)\n",
    "        learning_rate = self.learning_rate\n",
    "        weight_decay = self.weight_decay\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, eps=weight_decay)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            train_loss, train_acc = self.QA_model_train(model,device,optimizer,loss_func,train,epoch)\n",
    "            valid_loss, valid_acc = self.QA_evaluate(model,device,loss_func,val)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_losses.extend(valid_loss)\n",
    "            valid_accs.extend(valid_acc)\n",
    "    \n",
    "            epoch_valid_loss = np.mean(valid_loss)\n",
    "            epoch_valid_acc = np.mean(valid_acc)\n",
    "            \n",
    "            if epoch_valid_acc > best_valid_acc:\n",
    "                best_valid_acc = epoch_valid_acc\n",
    "                torch.save(model.state_dict(), 'QA_Best_Model.pt')\n",
    "            print('\\n')\n",
    "            print(f'epoch: {epoch+1}')\n",
    "            print(f'train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}')\n",
    "            print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
    "\n",
    "    def QA_Test(self,eval):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        new_model = QA_Model.from_pretrained(self.model_ver,num_labels=self.num_labels,hidden_size=self.hidden_size,hidden_dropout_prob=self.hidden_dropout_prob,max_length=self.max_length)\n",
    "        loaded_state_dict = torch.load('QA_Best_Model.pt',map_location=torch.device('cpu'))\n",
    "        remove_module_state_dict = {}\n",
    "        for key in loaded_state_dict.keys():\n",
    "            key_remove_module = re.sub('module.','',key)\n",
    "            value = loaded_state_dict[key]\n",
    "            remove_module_state_dict[key_remove_module] = value\n",
    "        new_model.load_state_dict(remove_module_state_dict)\n",
    "        new_model = new_model.to(device)\n",
    "        eval_loss, eval_acc = self.QA_evaluate(new_model,device,loss_func,eval)\n",
    "        epoch_test_loss = np.mean(eval_loss)\n",
    "        epoch_test_acc = np.mean(eval_acc)\n",
    "\n",
    "        print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
    "\n",
    "# roberta\n",
    "class QA_trainer1():\n",
    "    def __init__(self,num_labels,hidden_size,max_length,hidden_dropout_prob,batch_size,epoch,model_ver,learning_rate,weight_decay):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.model_ver = model_ver\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = QA_Model1.from_pretrained(self.model_ver,num_labels=self.num_labels,hidden_size=self.hidden_size,hidden_dropout_prob=self.hidden_dropout_prob,max_length=self.max_length)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "        self.sep_token = self.tokenizer.sep_token\n",
    "        \n",
    "    def make_dataset(self):\n",
    "        '''\n",
    "            data =\n",
    "            [\n",
    "            sentence_id : str\n",
    "            Context : str\n",
    "            Question : str\n",
    "            ( True : 1 or False : 0 ) : int \n",
    "            ]\n",
    "        '''\n",
    "        train_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Train.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                last_data = int(re.sub('\\n','',info.split('\\t')[-1]))\n",
    "                pre_data.append(last_data)\n",
    "                train_data.append(pre_data)\n",
    "        val_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Dev.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                last_data = int(re.sub('\\n','',info.split('\\t')[-1]))\n",
    "                pre_data.append(last_data)\n",
    "                val_data.append(pre_data)\n",
    "        test_data = []\n",
    "        with open('./data/task4/SKT_BoolQ_Test.tsv',encoding='UTF8') as f:\n",
    "            infos = f.readlines()\n",
    "            for info in infos[1:]:\n",
    "                pre_data = info.split('\\t')[:-1]\n",
    "                test_data.append(pre_data)\n",
    "        #train_data\n",
    "        train_data_context = [data[1] for data in train_data]\n",
    "        for n, context in enumerate(train_data_context):\n",
    "            sen_list = context.split('.')\n",
    "            new_context = ''\n",
    "            if len(sen_list)>=2:\n",
    "                for sen in sen_list[:-2]:\n",
    "                    new_context = new_context+sen+'.'+' [CLS]'\n",
    "                new_context = new_context+sen_list[-2]+'.'\n",
    "                train_data_context[n] = new_context\n",
    "        train_data_question = [data[2] for data in train_data]\n",
    "        train_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([]),'answer' : torch.tensor([])}\n",
    "        for data in range(len(train_data_context)):\n",
    "            train_data_tokenized = self.tokenizer.encode_plus(train_data_context[data],train_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = train_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = train_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = train_data_tokenized['token_type_ids']\n",
    "            train_result['input_ids'] = torch.cat([train_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            train_result['attention_mask'] = torch.cat([train_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            train_result['token_type_ids'] = torch.cat([train_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        train_result['input_ids'] = train_result['input_ids'].long()\n",
    "        train_result['attention_mask'] = train_result['attention_mask'].long()\n",
    "        train_result['token_type_ids'] = train_result['token_type_ids'].long()\n",
    "        train_data_answer = torch.tensor([data[3] for data in train_data])\n",
    "        train_result['answer'] = train_data_answer\n",
    "\n",
    "        #val_data\n",
    "        val_data_context = [data[1] for data in val_data]\n",
    "        for n, context in enumerate(val_data_context):\n",
    "            sen_list = context.split('.')\n",
    "            new_context = ''\n",
    "            if len(sen_list)>=2:\n",
    "                for sen in sen_list[:-2]:\n",
    "                    new_context = new_context+sen+'.'+' [CLS]'\n",
    "                new_context = new_context+sen_list[-2]+'.'\n",
    "                val_data_context[n] = new_context\n",
    "        val_data_question = [data[2] for data in val_data]\n",
    "        val_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([]),'answer' : torch.tensor([])}\n",
    "        for data in range(len(val_data_context)):\n",
    "            val_data_tokenized = self.tokenizer.encode_plus(val_data_context[data],val_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = val_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = val_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = val_data_tokenized['token_type_ids']\n",
    "            val_result['input_ids'] = torch.cat([val_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            val_result['attention_mask'] = torch.cat([val_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            val_result['token_type_ids'] = torch.cat([val_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        val_result['input_ids'] = val_result['input_ids'].long()\n",
    "        val_result['attention_mask'] = val_result['attention_mask'].long()\n",
    "        val_result['token_type_ids'] = val_result['token_type_ids'].long()\n",
    "        val_data_answer = torch.tensor([data[3] for data in val_data])\n",
    "        val_result['answer'] = val_data_answer\n",
    "\n",
    "        #test_data\n",
    "        test_data_context = [data[1] for data in test_data]\n",
    "        for n, context in enumerate(test_data_context):\n",
    "            sen_list = context.split('.')\n",
    "            new_context = ''\n",
    "            if len(sen_list)>=2:\n",
    "                for sen in sen_list[:-2]:\n",
    "                    new_context = new_context+sen+'.'+' [CLS]'\n",
    "                new_context = new_context+sen_list[-2]+'.'\n",
    "                test_data_context[n] = new_context\n",
    "        test_data_question = [data[2] for data in test_data]\n",
    "        test_result = {'input_ids' : torch.tensor([]) , 'attention_mask' : torch.tensor([]),'token_type_ids': torch.tensor([])}\n",
    "        for data in range(len(test_data_context)):\n",
    "            test_data_tokenized = self.tokenizer.encode_plus(test_data_context[data],test_data_question[data],return_token_type_ids=True,max_length= self.max_length, padding ='max_length', return_attention_mask=True, truncation=True,return_tensors='pt' )\n",
    "            truncated_input_ids = test_data_tokenized['input_ids']\n",
    "            truncated_attention_masks = test_data_tokenized['attention_mask']\n",
    "            truncated_token_type_ids = test_data_tokenized['token_type_ids']\n",
    "            test_result['input_ids'] = torch.cat([test_result['input_ids'], truncated_input_ids], dim = 0) \n",
    "            test_result['attention_mask'] = torch.cat([test_result['attention_mask'], truncated_attention_masks], dim = 0)\n",
    "            test_result['token_type_ids'] = torch.cat([test_result['token_type_ids'], truncated_token_type_ids], dim = 0)\n",
    "        test_result['input_ids'] = test_result['input_ids'].long()\n",
    "        test_result['attention_mask'] = test_result['attention_mask'].long()\n",
    "        test_result['token_type_ids'] = test_result['token_type_ids'].long()\n",
    "\n",
    "        train_dataset = TensorDataset(train_result[\"input_ids\"], train_result[\"attention_mask\"],train_result['token_type_ids'],train_result['answer'])\n",
    "        val_dataset = TensorDataset(val_result[\"input_ids\"], val_result[\"attention_mask\"],val_result['token_type_ids'],val_result['answer'])\n",
    "        test_dataset = TensorDataset(test_result[\"input_ids\"], test_result[\"attention_mask\"],test_result['token_type_ids'])\n",
    "\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        return train_data_loader, val_data_loader, test_data_loader\n",
    "\n",
    "    def accuracy(self,predict,label):\n",
    "        predict_answer = predict.argmax(dim=-1)\n",
    "        correct = predict_answer.eq(label.view_as(predict_answer)).sum()\n",
    "        return correct.float() / label.shape[0]\n",
    "\n",
    "    def QA_evaluate(self,model,device,loss_func,data):\n",
    "        model.eval()\n",
    "        epoch_losses = []\n",
    "        epoch_accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in data:\n",
    "            # for batch in tqdm(data,desc='dev_batch'):\n",
    "                input_id = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                token_type_ids = batch[2].to(device)\n",
    "                answer = batch[3].to(device)\n",
    "                predictions = model(input_ids=input_id, attention_mask = attention_mask,token_type_ids=token_type_ids)\n",
    "                loss = loss_func(predictions,answer)\n",
    "                acc = self.accuracy(predictions, answer)\n",
    "                epoch_losses.append(loss.item())\n",
    "                epoch_accs.append(acc.item())\n",
    "        return epoch_losses, epoch_accs\n",
    "\n",
    "    def QA_model_train(self,model,device,optimizer,loss_func,train,print_epoch): #validation 추가\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "        for batch in train:\n",
    "        # for batch in tqdm(train,desc='train_batch'):\n",
    "            optimizer.zero_grad()\n",
    "            input_id = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            token_type_ids = batch[2].to(device)\n",
    "            answer = batch[3].to(device)\n",
    "            predictions = model(input_ids=input_id, attention_mask = attention_mask,token_type_ids=token_type_ids)\n",
    "            loss = loss_func(predictions,answer)\n",
    "            acc = self.accuracy(predictions, answer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        train_loss = epoch_loss / len(train)\n",
    "        train_acc = epoch_acc / len(train)\n",
    "        # print(f'Epoch: {print_epoch+1:02}')\n",
    "        # print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        return train_loss,train_acc\n",
    "\n",
    "    def QA_Train(self,train,val):\n",
    "        epochs = self.epoch\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        valid_losses = []\n",
    "        valid_accs = []\n",
    "        best_valid_acc = float('-inf')\n",
    "        model = self.model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = DataParallel(model)\n",
    "        model = model.to(device)\n",
    "        learning_rate = self.learning_rate\n",
    "        weight_decay = self.weight_decay\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, eps=weight_decay)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            train_loss, train_acc = self.QA_model_train(model,device,optimizer,loss_func,train,epoch)\n",
    "            valid_loss, valid_acc = self.QA_evaluate(model,device,loss_func,val)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_losses.extend(valid_loss)\n",
    "            valid_accs.extend(valid_acc)\n",
    "    \n",
    "            epoch_valid_loss = np.mean(valid_loss)\n",
    "            epoch_valid_acc = np.mean(valid_acc)\n",
    "            \n",
    "            if epoch_valid_acc > best_valid_acc:\n",
    "                best_valid_acc = epoch_valid_acc\n",
    "                torch.save(model.state_dict(), 'QA_Best_Model.pt')\n",
    "            print('\\n')\n",
    "            print(f'epoch: {epoch+1}')\n",
    "            print(f'train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}')\n",
    "            print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
    "\n",
    "    def QA_Test(self,eval):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        new_model = QA_Model.from_pretrained(self.model_ver,num_labels=self.num_labels,hidden_size=self.hidden_size,hidden_dropout_prob=self.hidden_dropout_prob,max_length=self.max_length)\n",
    "        loaded_state_dict = torch.load('QA_Best_Model.pt',map_location=torch.device('cpu'))\n",
    "        remove_module_state_dict = {}\n",
    "        for key in loaded_state_dict.keys():\n",
    "            key_remove_module = re.sub('module.','',key)\n",
    "            value = loaded_state_dict[key]\n",
    "            remove_module_state_dict[key_remove_module] = value\n",
    "        new_model.load_state_dict(remove_module_state_dict)\n",
    "        new_model = new_model.to(device)\n",
    "        eval_loss, eval_acc = self.QA_evaluate(new_model,device,loss_func,eval)\n",
    "        epoch_test_loss = np.mean(eval_loss)\n",
    "        epoch_test_acc = np.mean(eval_acc)\n",
    "\n",
    "        print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training():\n",
    "    trainer = QA_trainer(num_labels=2,hidden_size=768,hidden_dropout_prob=0.0,\n",
    "                         max_length=512,batch_size=2,epoch=10,model_ver='monologg/koelectra-base-v3-discriminator',learning_rate=5e-6,weight_decay=5e-9)\n",
    "    train,val,test = trainer.make_dataset()\n",
    "    trainer.QA_Train(train,val)\n",
    "    trainer.QA_Test(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing QA_Model: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing QA_Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QA_Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of QA_Model were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight', 'classifier.dense.bias', 'dense.weight', 'dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 1/10 [06:08<55:12, 368.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 1\n",
      "train_loss: 0.630, train_acc: 0.614\n",
      "valid_loss: 0.491, valid_acc: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|██        | 2/10 [12:21<49:16, 369.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 2\n",
      "train_loss: 0.347, train_acc: 0.855\n",
      "valid_loss: 0.508, valid_acc: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|███       | 3/10 [18:31<43:08, 369.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 3\n",
      "train_loss: 0.186, train_acc: 0.934\n",
      "valid_loss: 0.638, valid_acc: 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████      | 4/10 [24:44<37:04, 370.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 4\n",
      "train_loss: 0.108, train_acc: 0.965\n",
      "valid_loss: 0.723, valid_acc: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|█████     | 5/10 [30:55<30:53, 370.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 5\n",
      "train_loss: 0.072, train_acc: 0.979\n",
      "valid_loss: 0.777, valid_acc: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|██████    | 6/10 [37:07<24:45, 371.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 6\n",
      "train_loss: 0.053, train_acc: 0.985\n",
      "valid_loss: 0.827, valid_acc: 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|███████   | 7/10 [43:47<18:59, 379.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 7\n",
      "train_loss: 0.044, train_acc: 0.985\n",
      "valid_loss: 0.964, valid_acc: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████  | 8/10 [49:57<12:33, 376.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 8\n",
      "train_loss: 0.035, train_acc: 0.990\n",
      "valid_loss: 0.909, valid_acc: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|█████████ | 9/10 [56:08<06:14, 375.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 9\n",
      "train_loss: 0.024, train_acc: 0.993\n",
      "valid_loss: 1.024, valid_acc: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:02:21<00:00, 374.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch: 10\n",
      "train_loss: 0.020, train_acc: 0.994\n",
      "valid_loss: 1.001, valid_acc: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing QA_Model: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing QA_Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QA_Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of QA_Model were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight', 'classifier.dense.bias', 'dense.weight', 'dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.001, test_acc: 0.817\n"
     ]
    }
   ],
   "source": [
    "Training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing QA_Model: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing QA_Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QA_Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of QA_Model were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.out_proj.weight', 'dense.weight', 'dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "new_model = QA_Model.from_pretrained('monologg/koelectra-base-v3-discriminator',num_labels=2,hidden_size=768,hidden_dropout_prob=0.0,max_length=512)\n",
    "loaded_state_dict = torch.load('./model/task4_best_model.pt',map_location=torch.device('cpu'))\n",
    "# new_model = QA_Model1.from_pretrained('klue/roberta-large',num_labels=2,hidden_size=768,hidden_dropout_prob=0.0,max_length=512)\n",
    "# loaded_state_dict = torch.load('./QA_Best_Model.pt',map_location=torch.device('cpu'))\n",
    "remove_module_state_dict = {}\n",
    "for key in loaded_state_dict.keys():\n",
    "    key_remove_module = re.sub('module.','',key)\n",
    "    value = loaded_state_dict[key]\n",
    "    remove_module_state_dict[key_remove_module] = value\n",
    "new_model.load_state_dict(remove_module_state_dict)\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing QA_Model: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing QA_Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QA_Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of QA_Model were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.out_proj.weight', 'dense.weight', 'dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.881, test_acc: 0.824\n"
     ]
    }
   ],
   "source": [
    "trainer = QA_trainer(num_labels=2,hidden_size=768,hidden_dropout_prob=0.0,\n",
    "                         max_length=512,batch_size=4,epoch=8,model_ver='monologg/koelectra-base-v3-discriminator',learning_rate=1e-5,weight_decay=5e-9)\n",
    "# trainer = QA_trainer1(num_labels=2,hidden_size=768,hidden_dropout_prob=0.0,\n",
    "#                          max_length=512,batch_size=4,epoch=8,model_ver='klue/roberta-large',learning_rate=1e-5,weight_decay=5e-9)\n",
    "train,val,test = trainer.make_dataset()\n",
    "eval_loss, eval_acc = trainer.QA_evaluate(new_model,device,loss_func,val)\n",
    "epoch_test_loss = np.mean(eval_loss)\n",
    "epoch_test_acc = np.mean(eval_acc)\n",
    "\n",
    "print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
