{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: 2021년 국립국어원 인공지능 언어능력 평가\n",
    "\n",
    "- [2021년 국립국어원 인공지능 언어능력 평가](https://corpus.korean.go.kr/task/taskList.do?taskId=1&clCd=END_TASK&subMenuId=sub01) 는 9월 1일부터 시작하여 11월 1일까지 마감된 [네 가지 과제에](https://corpus.korean.go.kr/task/taskDownload.do?taskId=1&clCd=END_TASK&subMenuId=sub02) 대한 언어능력 평가 대회\n",
    "- 여기서 제시된 과제를 그대로 수행하여 그 결과를 [최종 선정된 결과들](https://corpus.korean.go.kr/task/taskLeaderBoard.do?taskId=4&clCd=END_TASK&subMenuId=sub04)과 비교할 수 있도록 수행\n",
    "- 아직 테스트 셋의 정답이 공식적으로 공개되고 있지 않아, 네 가지 과제의 자료에서 evaluation dataset으로 가지고 성능을 비교할 계획\n",
    "- 기말 발표전까지 정답셋이 공개될 경우 이 정답셋을 가지고 성능 검증\n",
    "- Transformers 기반 방법론, 신경망 등 각자 생각한 방법대로 구현 가능\n",
    "- 현재 대회기간이 종료되어 자료가 다운로드 가능하지 않으니 첨부된 자료 참조\n",
    "- 개인적으로 하거나 최대 두명까지 그룹 허용. \n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 FinalProject_[DS또는 CL]_학과_이름.ipynb\n",
    "- 마감 12월 6일(월) 23:59분까지.\n",
    "- 12월 7일, 9일 기말 발표 presentation 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리더보드\n",
    "\n",
    "- 최종발표전까지 각조는 각 태스크별 실행성능을 **시도된 여러 방법의 결과들을 지속적으로**  [리더보드](https://docs.google.com/spreadsheets/d/1-uenfp5GolpY2Gf0TsFbODvj585IIiFKp9fvYxcfgkY/edit#gid=0)에 해당 팀명(구성원 이름 포함)을 입력하여 공개하여야 함. \n",
    "- 최종 마감일에 이 순위와 실제 제출한 프로그램의 수행 결과를 비교하여 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer,AutoModel, RobertaPreTrainedModel, AutoConfig\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_len):\n",
    "        # dataframe으로 읽어오기\n",
    "        dataset = pd.read_csv(data_path, delimiter='\\t')\n",
    "        # label을 int로 변경\n",
    "        dataset.loc[(dataset['ANSWER'] == False), 'ANSWER'] = 0  #False => 0\n",
    "        dataset.loc[(dataset['ANSWER'] != False), 'ANSWER'] = 1  #True,Nan => 1\n",
    "        self.input_ids=[]\n",
    "        self.attention_mask = []\n",
    "        self.entity_1_mask = []\n",
    "        self.entity_2_mask = []\n",
    "        for target, s1, s2, label, s_s1, e_s1, s_s2, e_s2 in zip(dataset['Target'], dataset['SENTENCE1'], dataset['SENTENCE2'], dataset['ANSWER'], dataset['start_s1'], dataset['end_s1'], dataset['start_s2'], dataset['end_s2']):\n",
    "            sentence = s1[:s_s1]+' [SS1] '+s1[s_s1:e_s1]+' [ES1] '+s1[e_s1:]+' [SEP] '+s2[:s_s2]+' [SS2] '+s2[s_s2:e_s2]+' [ES2] '+s2[e_s2:]\n",
    "            input_ids, attention_mask = tokenizer(sentence, max_length= max_len, padding ='max_length', return_token_type_ids=False, return_attention_mask=True, return_tensors='pt').values()\n",
    "            entity_mask = (input_ids>=32000)[0]\n",
    "            entity_index_list = [i for i, value in enumerate(entity_mask) if value == True]\n",
    "            entity_1_mask = torch.tensor([[0]*entity_index_list[0]+[1]*(entity_index_list[1]-entity_index_list[0]+1)+[0]*(len(input_ids[0])-entity_index_list[1]-1)])\n",
    "            entity_2_mask = torch.tensor([[0]*entity_index_list[2]+[1]*(entity_index_list[3]-entity_index_list[2]+1)+[0]*(len(input_ids[0])-entity_index_list[3]-1)])\n",
    "            self.input_ids.append(input_ids)\n",
    "            self.attention_mask.append(attention_mask)\n",
    "            self.entity_1_mask.append(entity_1_mask)\n",
    "            self.entity_2_mask.append(entity_2_mask)\n",
    "        self.labels = [torch.tensor(np.int32(i)) for i in dataset.ANSWER]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.input_ids[i], self.attention_mask[i], self.entity_1_mask[i], self.entity_2_mask[i], self.labels[i]\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32004, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 및 tokenizer 불러오기\n",
    "device = torch.device(\"cuda:1\")\n",
    "# bertmodel = BertModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")  # KoELECTRA-Base-v3\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "            \"klue/roberta-large\",\n",
    "            num_labels= 2\n",
    "        )\n",
    "bertmodel = AutoModel.from_pretrained(\"klue/roberta-large\", config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", return_token_type_ids=False)\n",
    "\n",
    "# tokenizer에 special tokens 추가\n",
    "special_tokens_dict = {'additional_special_tokens': ['[SS1]','[ES1]','[SS2]','[ES2]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "bertmodel.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "#parameters\n",
    "max_len = 280\n",
    "batch_size = 2\n",
    "data_train = BERTDataset('./data/task2/NIKL_SKT_WiC_Train.tsv', tokenizer, max_len)\n",
    "data_dev = BERTDataset('./data/task2/NIKL_SKT_WiC_Dev.tsv', tokenizer, max_len)\n",
    "data_test = BERTDataset('./data/task2/NIKL_SKT_WiC_Test.tsv', tokenizer, max_len)\n",
    "\n",
    "train_sampler = RandomSampler(data_train)\n",
    "dev_sampler = RandomSampler(data_dev)\n",
    "test_sampler = RandomSampler(data_test)\n",
    "\n",
    "train_dataloader = DataLoader(data_train, sampler=train_sampler, batch_size=batch_size, num_workers=4)\n",
    "dev_dataloader = DataLoader(data_dev, sampler=dev_sampler, batch_size=batch_size, num_workers=4)\n",
    "test_dataloader = DataLoader(data_test, sampler=test_sampler, batch_size=batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCLayer\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Bert Model\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 1024,\n",
    "                 num_classes=2,   ##클래스 수 조정##\n",
    "                 dr_rate=0.1,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.sen_layer = FCLayer(hidden_size, hidden_size, dr_rate)\n",
    "        self.entity_1_layer = FCLayer(hidden_size, hidden_size, dr_rate)\n",
    "        self.entity_2_layer = FCLayer(hidden_size, hidden_size, dr_rate)\n",
    "        self.classifier = FCLayer(hidden_size*3, num_classes, dr_rate, use_activation=False)\n",
    "        self.dr_rate = dr_rate\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
    "        return avg_vector\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, entity_1_mask, entity_2_mask):\n",
    "        input_ids = input_ids.squeeze(dim=1)\n",
    "        attention_mask = attention_mask.squeeze(dim=1)\n",
    "        entity_1_mask = entity_1_mask.squeeze(dim=1)\n",
    "        entity_2_mask = entity_2_mask.squeeze(dim=1)\n",
    "        \n",
    "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        entity_1_h = self.entity_average(output[0], entity_1_mask)\n",
    "        entity_2_h = self.entity_average(output[0], entity_2_mask)\n",
    "        \n",
    "        sentence_h = self.sen_layer(output[1])\n",
    "        entity_1_h = self.entity_1_layer(entity_1_h)\n",
    "        entity_2_h = self.entity_1_layer(entity_2_h)\n",
    "        \n",
    "        out = torch.cat((sentence_h, entity_1_h, entity_2_h), 1)\n",
    "        out =self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate =  1e-5\n",
    "dr_rate = 0.4\n",
    "log_interval = 200\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 1\n",
    "weight_decay = 1e-2\n",
    "warmup_steps = 64\n",
    "gradient_accumulation_steps = 2\n",
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_epochs\n",
    "\n",
    "# BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=dr_rate).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# t_total = len(train_dataloader) * num_epochs\n",
    "# warmup_step = int(t_total * warmup_ratio)\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "sig = nn.Sigmoid()\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"acc\": acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d613a7d3a11a464abb18679c3f8c0502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.790594756603241\n",
      "epoch 1 batch id 201 loss 0.7467426061630249\n",
      "epoch 1 batch id 401 loss 1.3428559303283691\n",
      "epoch 1 batch id 601 loss 0.6542983651161194\n",
      "epoch 1 batch id 801 loss 0.9069172143936157\n",
      "epoch 1 batch id 1001 loss 1.4783623218536377\n",
      "epoch 1 batch id 1201 loss 0.06883548200130463\n",
      "epoch 1 batch id 1401 loss 0.5499914288520813\n",
      "epoch 1 batch id 1601 loss 1.4313926696777344\n",
      "epoch 1 batch id 1801 loss 1.196056842803955\n",
      "epoch 1 batch id 2001 loss 2.0780880451202393\n",
      "epoch 1 batch id 2201 loss 0.005443713627755642\n",
      "epoch 1 batch id 2401 loss 1.7908670902252197\n",
      "epoch 1 batch id 2601 loss 0.006554496008902788\n",
      "epoch 1 batch id 2801 loss 0.004062008578330278\n",
      "epoch 1 batch id 3001 loss 2.0045833587646484\n",
      "epoch 1 batch id 3201 loss 0.0041614375077188015\n",
      "epoch 1 batch id 3401 loss 0.06080068647861481\n",
      "epoch 1 batch id 3601 loss 0.002382589504122734\n",
      "epoch 1 batch id 3801 loss 0.0033015816006809473\n",
      "epoch 1 train acc {'acc': 0.7210893133711925}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73eea97dc3e4edb9915d743e260586f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 validation acc {'acc': 0.8404802744425386}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942c7444dd11488aa21804f648d64cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.00527550745755434\n",
      "epoch 2 batch id 201 loss 0.0036405175924301147\n",
      "epoch 2 batch id 401 loss 0.001819691970013082\n",
      "epoch 2 batch id 601 loss 0.006443818099796772\n",
      "epoch 2 batch id 801 loss 0.0024621961638331413\n",
      "epoch 2 batch id 1001 loss 0.002642020583152771\n",
      "epoch 2 batch id 1201 loss 0.005969279911369085\n",
      "epoch 2 batch id 1401 loss 0.011766495183110237\n",
      "epoch 2 batch id 1601 loss 0.0028272320050746202\n",
      "epoch 2 batch id 1801 loss 0.0035058900248259306\n",
      "epoch 2 batch id 2001 loss 0.06369456648826599\n",
      "epoch 2 batch id 2201 loss 0.0006226729601621628\n",
      "epoch 2 batch id 2401 loss 2.837902069091797\n",
      "epoch 2 batch id 2601 loss 0.00880520511418581\n",
      "epoch 2 batch id 2801 loss 0.003517232835292816\n",
      "epoch 2 batch id 3001 loss 0.008019773289561272\n",
      "epoch 2 batch id 3201 loss 0.018939699977636337\n",
      "epoch 2 batch id 3401 loss 0.012873920612037182\n",
      "epoch 2 batch id 3601 loss 0.0026731425896286964\n",
      "epoch 2 batch id 3801 loss 0.0007746668998152018\n",
      "epoch 2 train acc {'acc': 0.8776458440887971}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462c38deafed49948e778f8c0eedf40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 validation acc {'acc': 0.8370497427101201}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48db91e1fc294785b1ce20db47be0a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.0005473165074363351\n",
      "epoch 3 batch id 201 loss 0.00044859369518235326\n",
      "epoch 3 batch id 401 loss 3.7474963665008545\n",
      "epoch 3 batch id 601 loss 0.00046206306433305144\n",
      "epoch 3 batch id 801 loss 0.000991150038316846\n",
      "epoch 3 batch id 1001 loss 0.0028611482121050358\n",
      "epoch 3 batch id 1201 loss 0.003729625139385462\n",
      "epoch 3 batch id 1401 loss 0.000587645685300231\n",
      "epoch 3 batch id 1601 loss 0.0007251350907608867\n",
      "epoch 3 batch id 1801 loss 0.0010655898367986083\n",
      "epoch 3 batch id 2001 loss 0.0026257354766130447\n",
      "epoch 3 batch id 2201 loss 0.00253859581425786\n",
      "epoch 3 batch id 2401 loss 0.001768719870597124\n",
      "epoch 3 batch id 2601 loss 2.9481699466705322\n",
      "epoch 3 batch id 2801 loss 0.0017502878326922655\n",
      "epoch 3 batch id 3001 loss 0.0007944323588162661\n",
      "epoch 3 batch id 3201 loss 0.0003815598611254245\n",
      "epoch 3 batch id 3401 loss 3.6257262229919434\n",
      "epoch 3 batch id 3601 loss 0.0021949734073132277\n",
      "epoch 3 batch id 3801 loss 0.0008049942553043365\n",
      "epoch 3 train acc {'acc': 0.9241094475993805}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8d88003689470f8236ceb3ccf6a0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 validation acc {'acc': 0.8739279588336192}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff02df2596248f09be912dfceeb83fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.0014271220425143838\n",
      "epoch 4 batch id 201 loss 0.0007870995323173702\n",
      "epoch 4 batch id 401 loss 0.0006624786765314639\n",
      "epoch 4 batch id 601 loss 3.163292646408081\n",
      "epoch 4 batch id 801 loss 0.0005943990545347333\n",
      "epoch 4 batch id 1001 loss 0.0007640764233656228\n",
      "epoch 4 batch id 1201 loss 0.001174368429929018\n",
      "epoch 4 batch id 1401 loss 0.015505165793001652\n",
      "epoch 4 batch id 1601 loss 0.0013265145244076848\n",
      "epoch 4 batch id 1801 loss 0.0012510617962107062\n",
      "epoch 4 batch id 2001 loss 0.0030161028262227774\n",
      "epoch 4 batch id 2201 loss 0.0004979298100806773\n",
      "epoch 4 batch id 2401 loss 0.01264047808945179\n",
      "epoch 4 batch id 2601 loss 0.0016506321262568235\n",
      "epoch 4 batch id 2801 loss 0.000547493458725512\n",
      "epoch 4 batch id 3001 loss 0.0004561647365335375\n",
      "epoch 4 batch id 3201 loss 0.0020360038615763187\n",
      "epoch 4 batch id 3401 loss 0.000336756173055619\n",
      "epoch 4 batch id 3601 loss 0.00023683741164859384\n",
      "epoch 4 batch id 3801 loss 0.00022957049077376723\n",
      "epoch 4 train acc {'acc': 0.9504388229220444}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a587741fce14b4fa9c27350ae83ad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 validation acc {'acc': 0.8876500857632933}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55da079ef3b74eb39dc1de602a445224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.01683790236711502\n",
      "epoch 5 batch id 201 loss 0.0002049551549134776\n",
      "epoch 5 batch id 401 loss 4.1446123123168945\n",
      "epoch 5 batch id 601 loss 0.0005619748844765127\n",
      "epoch 5 batch id 801 loss 0.0006912937969900668\n",
      "epoch 5 batch id 1001 loss 0.00044431054266169667\n",
      "epoch 5 batch id 1201 loss 0.000928266323171556\n",
      "epoch 5 batch id 1401 loss 0.000205674470635131\n",
      "epoch 5 batch id 1601 loss 0.00034805660834535956\n",
      "epoch 5 batch id 1801 loss 0.0006297264480963349\n",
      "epoch 5 batch id 2001 loss 0.0005475491052493453\n",
      "epoch 5 batch id 2201 loss 0.0006058131693862379\n",
      "epoch 5 batch id 2401 loss 0.0009276518831029534\n",
      "epoch 5 batch id 2601 loss 0.00020412176672834903\n",
      "epoch 5 batch id 2801 loss 0.00028772425139322877\n",
      "epoch 5 batch id 3001 loss 0.0016889076214283705\n",
      "epoch 5 batch id 3201 loss 0.0002923620049841702\n",
      "epoch 5 batch id 3401 loss 0.00034968130057677627\n",
      "epoch 5 batch id 3601 loss 0.0003647725097835064\n",
      "epoch 5 batch id 3801 loss 0.00019929301925003529\n",
      "epoch 5 train acc {'acc': 0.9650232318017553}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7267b5a8e347049dc4e80992dfacb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 validation acc {'acc': 0.902229845626072}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cdc3dc70a24c12a414ec0bb167e0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.0003923597978428006\n",
      "epoch 6 batch id 201 loss 0.0004294646787457168\n",
      "epoch 6 batch id 401 loss 0.00033953547244891524\n",
      "epoch 6 batch id 601 loss 0.00020238629076629877\n",
      "epoch 6 batch id 801 loss 0.0005185451591387391\n",
      "epoch 6 batch id 1001 loss 0.00011538515536813065\n",
      "epoch 6 batch id 1201 loss 8.171319495886564e-05\n",
      "epoch 6 batch id 1401 loss 0.0002086538588628173\n",
      "epoch 6 batch id 1601 loss 9.190299897454679e-05\n",
      "epoch 6 batch id 1801 loss 0.0003420373541302979\n",
      "epoch 6 batch id 2001 loss 0.00012265566329006106\n",
      "epoch 6 batch id 2201 loss 0.0005048395833000541\n",
      "epoch 6 batch id 2401 loss 0.0003611051070038229\n",
      "epoch 6 batch id 2601 loss 0.0007300837314687669\n",
      "epoch 6 batch id 2801 loss 3.683489194372669e-05\n",
      "epoch 6 batch id 3001 loss 0.0007906716782599688\n",
      "epoch 6 batch id 3201 loss 0.0006224270327948034\n",
      "epoch 6 batch id 3401 loss 0.0002892651245929301\n",
      "epoch 6 batch id 3601 loss 0.0002751533465925604\n",
      "epoch 6 batch id 3801 loss 0.0002878435770981014\n",
      "epoch 6 train acc {'acc': 0.9762519359834796}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4c573f87ab455aa43ccfabeee5bef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 validation acc {'acc': 0.8970840480274442}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a1811a88c14881a84cbfca2279b268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.0001326664787484333\n",
      "epoch 7 batch id 201 loss 0.00013541101361624897\n",
      "epoch 7 batch id 401 loss 0.0007587043801322579\n",
      "epoch 7 batch id 601 loss 0.00023868808057159185\n",
      "epoch 7 batch id 801 loss 0.00015031162183731794\n",
      "epoch 7 batch id 1001 loss 0.00022276813979260623\n",
      "epoch 7 batch id 1201 loss 0.0001829663524404168\n",
      "epoch 7 batch id 1401 loss 0.00028347159968689084\n",
      "epoch 7 batch id 1601 loss 0.00016300519928336143\n",
      "epoch 7 batch id 1801 loss 0.0004114249022677541\n",
      "epoch 7 batch id 2001 loss 7.360887684626505e-05\n",
      "epoch 7 batch id 2201 loss 6.764876161469147e-05\n",
      "epoch 7 batch id 2401 loss 0.0002599376894067973\n",
      "epoch 7 batch id 2601 loss 0.00012748473091050982\n",
      "epoch 7 batch id 2801 loss 7.045007077977061e-05\n",
      "epoch 7 batch id 3001 loss 0.00011705466022249311\n",
      "epoch 7 batch id 3201 loss 0.00049797841347754\n",
      "epoch 7 batch id 3401 loss 2.6778483390808105\n",
      "epoch 7 batch id 3601 loss 0.00017986400052905083\n",
      "epoch 7 batch id 3801 loss 0.00034320293343625963\n",
      "epoch 7 train acc {'acc': 0.9861899845121321}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b35e79fdf648bf92ee6592bed17f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 validation acc {'acc': 0.9159519725557461}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db58191c4c2e4bc8b176b127864bb593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.00013111824227962643\n",
      "epoch 8 batch id 201 loss 9.798243991099298e-05\n",
      "epoch 8 batch id 401 loss 4.482168878894299e-05\n",
      "epoch 8 batch id 601 loss 0.00020835487521253526\n",
      "epoch 8 batch id 801 loss 5.948278703726828e-05\n",
      "epoch 8 batch id 1001 loss 3.540433681337163e-05\n",
      "epoch 8 batch id 1201 loss 0.0002613885444588959\n",
      "epoch 8 batch id 1401 loss 0.00015495916886720806\n",
      "epoch 8 batch id 1601 loss 3.367601311765611e-05\n",
      "epoch 8 batch id 1801 loss 2.4318376745213754e-05\n",
      "epoch 8 batch id 2001 loss 0.00013863072672393173\n",
      "epoch 8 batch id 2201 loss 7.688695040997118e-05\n",
      "epoch 8 batch id 2401 loss 4.291442019166425e-05\n",
      "epoch 8 batch id 2601 loss 6.40722646494396e-05\n",
      "epoch 8 batch id 2801 loss 3.9636190194869414e-05\n",
      "epoch 8 batch id 3001 loss 9.470416989643127e-05\n",
      "epoch 8 batch id 3201 loss 6.043647226761095e-05\n",
      "epoch 8 batch id 3401 loss 8.814987813821062e-05\n",
      "epoch 8 batch id 3601 loss 5.995970423100516e-05\n",
      "epoch 8 batch id 3801 loss 0.00010203722195001319\n",
      "epoch 8 train acc {'acc': 0.9901910170366546}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fb4ba91e6f47728a3c053172033f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 validation acc {'acc': 0.9065180102915952}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3595fa3fb1411492c3dd4bfc5008a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 4.291411823942326e-05\n",
      "epoch 9 batch id 201 loss 5.906618753215298e-05\n",
      "epoch 9 batch id 401 loss 2.8252092306502163e-05\n",
      "epoch 9 batch id 601 loss 0.00013290331116877496\n",
      "epoch 9 batch id 801 loss 1.7464008124079555e-05\n",
      "epoch 9 batch id 1001 loss 3.296063368907198e-05\n",
      "epoch 9 batch id 1201 loss 3.069591912208125e-05\n",
      "epoch 9 batch id 1401 loss 3.290121821919456e-05\n",
      "epoch 9 batch id 1601 loss 0.00010966559057123959\n",
      "epoch 9 batch id 1801 loss 5.858963413629681e-05\n",
      "epoch 9 batch id 2001 loss 4.1483770473860204e-05\n",
      "epoch 9 batch id 2201 loss 7.557579374406487e-05\n",
      "epoch 9 batch id 2401 loss 2.5093217118410394e-05\n",
      "epoch 9 batch id 2601 loss 3.92783222196158e-05\n",
      "epoch 9 batch id 2801 loss 1.609310129424557e-05\n",
      "epoch 9 batch id 3001 loss 0.0001037039837683551\n",
      "epoch 9 batch id 3201 loss 0.00013404031051322818\n",
      "epoch 9 batch id 3401 loss 5.3582709369948134e-05\n",
      "epoch 9 batch id 3601 loss 1.1444026313256472e-05\n",
      "epoch 9 batch id 3801 loss 0.0001267710467800498\n",
      "epoch 9 train acc {'acc': 0.9923851316468766}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b61cb436f4261bef81c3b5bf6f085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 validation acc {'acc': 0.8996569468267581}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c6b9c113c240f19ca024cc33467ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 1.5497076674364507e-05\n",
      "epoch 10 batch id 201 loss 0.00014500756515190005\n",
      "epoch 10 batch id 401 loss 0.0001320105220656842\n",
      "epoch 10 batch id 601 loss 1.4006974197400268e-05\n",
      "epoch 10 batch id 801 loss 7.140310481190681e-05\n",
      "epoch 10 batch id 1001 loss 7.086715777404606e-05\n",
      "epoch 10 batch id 1201 loss 5.715917359339073e-05\n",
      "epoch 10 batch id 1401 loss 8.77934944583103e-05\n",
      "epoch 10 batch id 1601 loss 1.8596445443108678e-05\n",
      "epoch 10 batch id 1801 loss 2.9027040000073612e-05\n",
      "epoch 10 batch id 2001 loss 4.535799234872684e-05\n",
      "epoch 10 batch id 2201 loss 1.9788545614574105e-05\n",
      "epoch 10 batch id 2401 loss 3.647694393293932e-05\n",
      "epoch 10 batch id 2601 loss 6.669516733381897e-05\n",
      "epoch 10 batch id 2801 loss 6.031803786754608e-05\n",
      "epoch 10 batch id 3001 loss 7.694640953559428e-05\n",
      "epoch 10 batch id 3201 loss 1.2338080523477402e-05\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "for e in range(num_epochs):\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, attention_mask, entity_1_mask, entity_2_mask, label) in enumerate(notebook.tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # load input to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        entity_1_mask = entity_1_mask.to(device)\n",
    "        entity_2_mask = entity_2_mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # model output\n",
    "        out = model(input_ids, attention_mask, entity_1_mask, entity_2_mask)\n",
    "        loss = loss_fn(out.view(-1, 2), label.long().view(-1))\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        # optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate schedule\n",
    "        if (batch_id + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()            \n",
    "        \n",
    "        # calc accuracy\n",
    "        if preds is None:\n",
    "                preds = out.detach().cpu().numpy()\n",
    "                out_label_ids = label.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, out.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, label.detach().cpu().numpy(), axis=0)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {}\".format(e+1, batch_id+1, loss.data.cpu().numpy()))\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    print(\"epoch {} train acc {}\".format(e+1, result))\n",
    "    torch.save(model, 'model_1_{}_{}.pt'.format(e+1, result))\n",
    "    \n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids, attention_mask, entity_1_mask, entity_2_mask, label) in enumerate(notebook.tqdm(dev_dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        entity_1_mask = entity_1_mask.to(device)\n",
    "        entity_2_mask = entity_2_mask.to(device)\n",
    "        label = label.long().to(device)\n",
    "        \n",
    "        # model output\n",
    "        out = model(input_ids, attention_mask, entity_1_mask, entity_2_mask)\n",
    "        \n",
    "        # calc accuracy\n",
    "        if preds is None:\n",
    "                preds = out.detach().cpu().numpy()\n",
    "                out_label_ids = label.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, out.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, label.detach().cpu().numpy(), axis=0)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    print(\"epoch {} validation acc {}\".format(e+1, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "### Test Data로 성능 확인시 다음 셀의 반복문에서 dev dataloader를 test dataloader로 코드 수정 필요합니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ec9d2cb9434133a4cfd85e38ff4505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model task2_best_model(1).pt validation acc {'acc': 0.9159519725557461}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8339e1759f489c8b6f440e02d2b784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model task2_best_model(2).pt validation acc {'acc': 0.9108061749571184}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd192edf1d3401ca5b6efa9dd723e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model task2_best_model(3).pt validation acc {'acc': 0.9116638078902229}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32df6607d964469ba3a2c9025fcddd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model task2_best_model(4).pt validation acc {'acc': 0.9159519725557461}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b08e313c684cbeb49b17f37a2d04de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model task2_best_model(5).pt validation acc {'acc': 0.9039451114922813}\n"
     ]
    }
   ],
   "source": [
    "# Test 진행\n",
    "#정확도 측정을 위한 함수 정의\n",
    "models_to_ensemble = ['task2_best_model(1).pt',\n",
    "                      'task2_best_model(2).pt',\n",
    "                      'task2_best_model(3).pt',\n",
    "                      'task2_best_model(4).pt',\n",
    "                      'task2_best_model(5).pt',]\n",
    "ensemble_preds = {}\n",
    "\n",
    "dev_dataloader = DataLoader(data_dev, batch_size=batch_size, num_workers=4)\n",
    "test_dataloader = DataLoader(data_test, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"acc\": acc,}\n",
    "\n",
    "for model_name in models_to_ensemble:\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model = BERTClassifier(bertmodel).to(device)\n",
    "    model = torch.load('./model/'+model_name).to(device)\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids, attention_mask, entity_1_mask, entity_2_mask, label) in enumerate(notebook.tqdm(dev_dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        entity_1_mask = entity_1_mask.to(device)\n",
    "        entity_2_mask = entity_2_mask.to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        # model output\n",
    "        out = model(input_ids, attention_mask, entity_1_mask, entity_2_mask)\n",
    "\n",
    "        # calc accuracy\n",
    "        if preds is None:\n",
    "            preds = out.detach().cpu().numpy()\n",
    "            out_label_ids = label.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, out.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, label.detach().cpu().numpy(), axis=0)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    ensemble_preds[model_name] = preds\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    print(\"model {} validation acc {}\".format(model_name, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ensemble validation acc {'acc': 0.9322469982847341}\n"
     ]
    }
   ],
   "source": [
    "# weighted 앙상블 성능\n",
    "# weight는 임의로 변경하며 탐색함\n",
    "weight = [2, 2, 1, 3, 1]\n",
    "\n",
    "for n, i in enumerate(ensemble_preds.values()):\n",
    "    if n == 0:\n",
    "        preds = weight[n] * i\n",
    "    else:\n",
    "        preds = preds + weight[n] * i\n",
    "        # pred = np.around(preds/sum(weight[:n+1]))\n",
    "        # result = compute_metrics(pred, out_label_ids)\n",
    "        # print(\"model ensemble validation acc {}\".format(result))\n",
    "pred = np.around(preds/sum(weight[:n+1]))\n",
    "result = compute_metrics(pred, out_label_ids)\n",
    "print(\"model ensemble validation acc {}\".format(result))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
